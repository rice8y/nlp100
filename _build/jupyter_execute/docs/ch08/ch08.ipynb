{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として, ニューラルネットワークで分類モデルを実装する.なお, この章ではPyTorchやTensorFlow, JAXなどの深層学習フレームワークを活用せよ.\n",
    "\n",
    "```{warning}\n",
    "本章は, `code-cell` ではなく, Markdown のコードブロック内にコードを記述しているため, Google Colab上で直接実行できません.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "\n",
    "\n",
    "事前学習済み単語埋め込みを活用し, $|V|\\times d_{\\text{emb}}$の単語埋め込み行列を作成せよ.ここで, $|V|$は単語埋め込みの語彙数, $d_{\\text{emb}}$は単語埋め込みの次元数である.ただし, 単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,;}$は, 将来的にパディング (\\<PAD\\>) トークンの埋め込みベクトルとして用いたいので, ゼロベクトルとして予約せよ.ゆえに, $\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる.\n",
    "\n",
    "もし, Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) (300万単語・フレーズ, 300次元) を全て読み込んだ場合, $|V|=3000001$, $d_{\\text{emb}}=300$になるはずである (ただ, 300万単語の中には, 殆ど用いられない稀な単語も含まれるので, 語彙を削減した方がメモリの節約になる).\n",
    "\n",
    "\n",
    "また, 単語埋め込み行列の構築と同時に, 単語埋め込み行列の各行のインデックス番号 (トークンID) と, 単語 (トークン) への双方向の対応付けを保持せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = model[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Embedding matrix shape: (3000001, 300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし, 訓練セット (train.tsv) と開発セット (dev.tsv) のテキストと極性ラベルと読み込み, 全てのテキストをトークンID列に変換せよ.このとき, 単語埋め込みの語彙でカバーされていない単語は無視し, トークン列に含めないことにせよ.また, テキストの全トークンが単語埋め込みの語彙に含まれておらず, 空のトークン列となってしまう事例は, 訓練セットおよび開発セットから削除せよ (このため, 第7章の実験で得られた正解率と比較できなくなることに注意せよ).\n",
    "\n",
    "事例の表現方法は任意でよいが, 例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は, 次のような辞書オブジェクトで表現すればよい.\n",
    "\n",
    "```py\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では, `text`はテキスト, `label`は分類ラベル (ポジティブなら`tensor([1.])`, ネガティブなら`tensor([0.])`), `input_ids`はテキストのトークン列をID列で表現している."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "print(f\"#train: {len(train_data)}, #dev: {len(dev_data)}\")\n",
    "print(\"Example: \", train_data[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#train: 66650, #dev: 872\n",
    "Example:  {'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し, 重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク (ロジスティック回帰モデル) を設計せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix), freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mean_embed = embedded.mean(dim=0)    \n",
    "        return self.linear(mean_embed)      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ.ただし, 学習中は単語埋め込み行列の値を固定せよ (単語埋め込み行列のファインチューニングは行わない) .また, 学習時に損失値を表示するなど, 学習の進捗状況をモニタリングできるようにせよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "token2id = {\"<PAD>\": 0}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "\n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        tokens = row[\"sentence\"].split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if not input_ids:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"label\": torch.tensor(float(row[\"label\"]), dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix), freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mean_embed = embedded.mean(dim=0)    \n",
    "        return self.linear(mean_embed)      \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MeanEmbeddingClassifier(embedding_matrix).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_data, dev_data):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for ex in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = ex[\"input_ids\"].to(device)\n",
    "        label = ex[\"label\"].to(device)\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits.squeeze(), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_loss = []\n",
    "    with torch.no_grad():\n",
    "        for ex in dev_data:\n",
    "            input_ids = ex[\"input_ids\"].to(device)\n",
    "            label = ex[\"label\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.squeeze(), label)\n",
    "            dev_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(train_loss), np.mean(dev_loss)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_l, dev_l = train_model(model, train_data, dev_data)\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"[Epoch {epoch}] Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./models/model_ex73.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    " 10%|████████████                                                                                                            | 1/10 [01:39<14:55, 99.51s/it][Epoch 2] Train loss: 0.5946, Dev loss: 0.6274\n",
    " 30%|████████████████████████████████████                                                                                    | 3/10 [04:34<10:30, 90.13s/it][Epoch 4] Train loss: 0.5255, Dev loss: 0.5876\n",
    " 50%|████████████████████████████████████████████████████████████                                                            | 5/10 [07:31<07:20, 88.08s/it][Epoch 6] Train loss: 0.4878, Dev loss: 0.5629\n",
    " 70%|████████████████████████████████████████████████████████████████████████████████████                                    | 7/10 [10:23<04:19, 86.41s/it][Epoch 8] Train loss: 0.4647, Dev loss: 0.5461\n",
    " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 9/10 [13:22<01:27, 87.97s/it][Epoch 10] Train loss: 0.4491, Dev loss: 0.5341\n",
    "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [14:34<00:00, 87.42s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "token2id = {\"<PAD>\": 0}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "\n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        tokens = row[\"sentence\"].split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if input_ids:\n",
    "            examples.append({\n",
    "                \"label\": torch.tensor(float(row[\"label\"]), dtype=torch.float32),\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "            })\n",
    "    return examples\n",
    "\n",
    "dev_data = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "dev_loader = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix), freeze=freeze_embedding)\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mean_embed = embedded.mean(dim=1)\n",
    "        return self.linear(mean_embed).squeeze(1)\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            preds = (torch.sigmoid(model(input_ids)) > 0.5).float()\n",
    "            all_preds.append(preds.item())\n",
    "            all_labels.append(labels.item())\n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MeanEmbeddingClassifier(embedding_matrix).to(device)\n",
    "model.load_state_dict(torch.load(\"./models/model_ex73.pt\"))\n",
    "\n",
    "test_acc = eval_model(model, dev_loader)\n",
    "print(f\"Dev ACC: {test_acc:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Dev ACC: 0.7752\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. パディング\n",
    "\n",
    "\n",
    "複数の事例が与えられたとき, これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ.与えられた複数の事例のトークン列の長さが異なるときは, トークン列の長さが最も長いものに揃え, 0番のトークンIDでパディングをせよ.さらに, トークン列の長さが長いものから順に, 事例を並び替えよ.\n",
    "\n",
    "例えば, 訓練データセットの冒頭の4事例が次のように表されているとき,\n",
    "\n",
    "```py\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される.\n",
    "\n",
    "```py\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}, {'text': 'contains no wit , only labored gags ', 'label': tensor([0.]), 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}, {'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': tensor([1.]), 'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,\n",
    "         1964])}]\n",
    "{'input_ids': tensor([[     75,    6355,     639,     165,     481,      75,       5],\n",
    "        [1057396,   12147,   10894,      66,     202,    1270,       0],\n",
    "        [    628,    1490,       0,       0,       0,       0,       0],\n",
    "        [  20839,       0,       0,       0,       0,       0,       0]]), 'label': tensor([[1.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "\n",
    "問題75のパディングの処理を活用して, ミニバッチでモデルを学習せよ.また, 学習したモデルの開発セットにおける正解率を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix), freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        mask = (input_ids != 0).unsqueeze(-1)\n",
    "        embedded = self.embedding(input_ids) * mask\n",
    "        mean_embed = embedded.sum(1) / mask.sum(1).clamp(min=1)\n",
    "        return self.linear(mean_embed).squeeze(1)\n",
    "    \n",
    "def collate(batch):\n",
    "    batch.sort(key=lambda x: len(x[\"input_ids\"]), reverse=True)\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    padded_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"input_ids\": padded_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MeanEmbeddingClassifier(embedding_matrix).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_loader, dev_loader):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            dev_batch_loss.append(loss.item())\n",
    "\n",
    "    train_acc = eval_model(model, train_loader)\n",
    "    dev_acc = eval_model(model, dev_loader)\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(dev_batch_loss), train_acc, dev_acc\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "epochs = 100\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model, train_l, dev_l, train_a, dev_a = train_model(model, train_loader, dev_loader)\n",
    "    train_loss.append(train_l)\n",
    "    dev_loss.append(dev_l)\n",
    "    train_acc.append(train_a)\n",
    "    dev_acc.append(dev_a)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"[Epoch {epoch + 1}]\")\n",
    "        print(f\"Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "        print(f\"Train acc : {train_a:.4f}, Dev acc : {dev_a:.4f}\")\n",
    "        \n",
    "path_saved_model = \"./models/model_ex76.pt\"\n",
    "torch.save(model.state_dict(), path_saved_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[Epoch 10]\n",
    "Train loss: 0.6315, Dev loss: 0.6625\n",
    "Train acc : 0.6616, Dev acc : 0.5436\n",
    "[Epoch 20]\n",
    "Train loss: 0.5855, Dev loss: 0.6358\n",
    "Train acc : 0.7450, Dev acc : 0.6239\n",
    "[Epoch 30]\n",
    "Train loss: 0.5503, Dev loss: 0.6132\n",
    "Train acc : 0.7857, Dev acc : 0.6984\n",
    "[Epoch 40]\n",
    "Train loss: 0.5231, Dev loss: 0.5952\n",
    "Train acc : 0.8018, Dev acc : 0.7339\n",
    "[Epoch 50]\n",
    "Train loss: 0.5019, Dev loss: 0.5782\n",
    "Train acc : 0.8100, Dev acc : 0.7569\n",
    "[Epoch 60]\n",
    "Train loss: 0.4852, Dev loss: 0.5662\n",
    "Train acc : 0.8145, Dev acc : 0.7569\n",
    "[Epoch 70]\n",
    "Train loss: 0.4717, Dev loss: 0.5562\n",
    "Train acc : 0.8177, Dev acc : 0.7649\n",
    "[Epoch 80]\n",
    "Train loss: 0.4608, Dev loss: 0.5459\n",
    "Train acc : 0.8206, Dev acc : 0.7706\n",
    "[Epoch 90]\n",
    "Train loss: 0.4516, Dev loss: 0.5410\n",
    "Train acc : 0.8221, Dev acc : 0.7775\n",
    "[Epoch 100]\n",
    "Train loss: 0.4439, Dev loss: 0.5328\n",
    "Train acc : 0.8237, Dev acc : 0.7775\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ.また, 学習したモデルの開発セットにおける正解率を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "問題76の解答と同様.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "\n",
    "問題77の学習において, 単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ.また, 学習したモデルの開発セットにおける正解率を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix), freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        mask = (input_ids != 0).unsqueeze(-1)\n",
    "        embedded = self.embedding(input_ids) * mask\n",
    "        mean_embed = embedded.sum(1) / mask.sum(1).clamp(min=1)\n",
    "        return self.linear(mean_embed).squeeze(1)\n",
    "    \n",
    "def collate(batch):\n",
    "    batch.sort(key=lambda x: len(x[\"input_ids\"]), reverse=True)\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    padded_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"input_ids\": padded_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MeanEmbeddingClassifier(embedding_matrix, freeze_embedding=False).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_loader, dev_loader):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            dev_batch_loss.append(loss.item())\n",
    "\n",
    "    train_acc = eval_model(model, train_loader)\n",
    "    dev_acc = eval_model(model, dev_loader)\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(dev_batch_loss), train_acc, dev_acc\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "epochs = 100\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model, train_l, dev_l, train_a, dev_a = train_model(model, train_loader, dev_loader)\n",
    "    train_loss.append(train_l)\n",
    "    dev_loss.append(dev_l)\n",
    "    train_acc.append(train_a)\n",
    "    dev_acc.append(dev_a)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"[Epoch {epoch + 1}]\")\n",
    "        print(f\"Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "        print(f\"Train acc : {train_a:.4f}, Dev acc : {dev_a:.4f}\")\n",
    "        \n",
    "path_saved_model = \"./models/model_ex78.pt\"\n",
    "torch.save(model.state_dict(), path_saved_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[Epoch 10]\n",
    "Train loss: 0.5800, Dev loss: 0.6252\n",
    "Train acc : 0.7714, Dev acc : 0.6548\n",
    "[Epoch 20]\n",
    "Train loss: 0.4502, Dev loss: 0.5404\n",
    "Train acc : 0.8561, Dev acc : 0.7603\n",
    "[Epoch 30]\n",
    "Train loss: 0.3604, Dev loss: 0.4802\n",
    "Train acc : 0.8805, Dev acc : 0.8073\n",
    "[Epoch 40]\n",
    "Train loss: 0.3066, Dev loss: 0.4413\n",
    "Train acc : 0.8945, Dev acc : 0.8211\n",
    "[Epoch 50]\n",
    "Train loss: 0.2724, Dev loss: 0.4249\n",
    "Train acc : 0.9039, Dev acc : 0.8222\n",
    "[Epoch 60]\n",
    "Train loss: 0.2485, Dev loss: 0.4159\n",
    "Train acc : 0.9121, Dev acc : 0.8257\n",
    "[Epoch 70]\n",
    "Train loss: 0.2308, Dev loss: 0.4122\n",
    "Train acc : 0.9179, Dev acc : 0.8234\n",
    "[Epoch 80]\n",
    "Train loss: 0.2167, Dev loss: 0.4100\n",
    "Train acc : 0.9229, Dev acc : 0.8211\n",
    "[Epoch 90]\n",
    "Train loss: 0.2054, Dev loss: 0.4107\n",
    "Train acc : 0.9265, Dev acc : 0.8177\n",
    "[Epoch 100]\n",
    "Train loss: 0.1960, Dev loss: 0.4144\n",
    "Train acc : 0.9297, Dev acc : 0.8131\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し, モデルを学習せよ.また, 学習したモデルの開発セットにおける正解率を求めよ.例えば, テキストの特徴ベクトル (単語埋め込みの平均ベクトル) に対して多層のニューラルネットワークを通したり, 畳み込みニューラルネットワーク (CNN; Convolutional Neural Network) や再帰型ニューラルネットワーク (RNN; Recurrent Neural Network) などのモデルの学習に挑戦するとよい."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True, num_filters=100, filter_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding\n",
    "        )\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                     out_channels=num_filters, \n",
    "                     kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_output = F.relu(conv(embedded))\n",
    "            pooled = F.max_pool1d(conv_output, conv_output.size(2))\n",
    "            conv_outputs.append(pooled.squeeze(2))\n",
    "\n",
    "        cat = torch.cat(conv_outputs, dim=1)\n",
    "        logits = self.fc(self.dropout(cat)).squeeze(1)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "def collate(batch):\n",
    "    batch.sort(key=lambda x: len(x[\"input_ids\"]), reverse=True)\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    padded_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"input_ids\": padded_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNNClassifier(embedding_matrix, freeze_embedding=False).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_loader, dev_loader):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).squeeze(1)\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            dev_batch_loss.append(loss.item())\n",
    "\n",
    "    train_acc = eval_model(model, train_loader)\n",
    "    dev_acc = eval_model(model, dev_loader)\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(dev_batch_loss), train_acc, dev_acc\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "epoch = 100\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    model, train_l, dev_l, train_a, dev_a = train_model(model, train_loader, dev_loader)\n",
    "    train_loss.append(train_l)\n",
    "    dev_loss.append(dev_l)\n",
    "    train_acc.append(train_a)\n",
    "    dev_acc.append(dev_a)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"[Epoch {epoch + 1}]\")\n",
    "        print(f\"Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "        print(f\"Train acc : {train_a:.4f}, Dev acc : {dev_a:.4f}\")\n",
    "        \n",
    "path_saved_model = \"./models/model_ex79.pt\"\n",
    "torch.save(model.state_dict(), path_saved_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[Epoch 10]\n",
    "Train loss: 0.2846, Dev loss: 0.4100\n",
    "Train acc : 0.8932, Dev acc : 0.8165\n",
    "[Epoch 20]\n",
    "Train loss: 0.2158, Dev loss: 0.4273\n",
    "Train acc : 0.9208, Dev acc : 0.8119\n",
    "[Epoch 30]\n",
    "Train loss: 0.1761, Dev loss: 0.4495\n",
    "Train acc : 0.9383, Dev acc : 0.8177\n",
    "[Epoch 40]\n",
    "Train loss: 0.1492, Dev loss: 0.4782\n",
    "Train acc : 0.9488, Dev acc : 0.8131\n",
    "[Epoch 50]\n",
    "Train loss: 0.1298, Dev loss: 0.5264\n",
    "Train acc : 0.9569, Dev acc : 0.8119\n",
    "[Epoch 60]\n",
    "Train loss: 0.1139, Dev loss: 0.5743\n",
    "Train acc : 0.9627, Dev acc : 0.8085\n",
    "[Epoch 70]\n",
    "Train loss: 0.1015, Dev loss: 0.6285\n",
    "Train acc : 0.9670, Dev acc : 0.8085\n",
    "[Epoch 80]\n",
    "Train loss: 0.0902, Dev loss: 0.6817\n",
    "Train acc : 0.9704, Dev acc : 0.8028\n",
    "[Epoch 90]\n",
    "Train loss: 0.0823, Dev loss: 0.7463\n",
    "Train acc : 0.9733, Dev acc : 0.7993\n",
    "[Epoch 100]\n",
    "Train loss: 0.0758, Dev loss: 0.7942\n",
    "Train acc : 0.9756, Dev acc : 0.7924\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}