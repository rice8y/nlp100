{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章: 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では, [Stanford Sentiment Treebank (SST)](https://nlp.stanford.edu/sentiment/) データセットを用い, 評判分析器 (ポジネガ分類器) を構築する. ここでは処理を簡略化するため, [General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されているSSTデータセットを用いる.\n",
    "\n",
    "```{warning}\n",
    "本章は, `code-cell` ではなく, Markdown のコードブロック内にコードを記述しているため, Google Colab上で直接実行できません.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. データの入手・整形\n",
    "\n",
    "GLUEのウェブサイトから[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)データセットを取得せよ。学習データ（`train.tsv`）と検証データ（`dev.tsv`）のぞれぞれについて、ポジティブ (1) とネガティブ (0) の事例数をカウントせよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
    "# !unzip SST-2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(\"# train.tsv\")\n",
    "print(f\"Positive: {(train_df[\"label\"]==1).sum()}\")\n",
    "print(f\"Negative: {(train_df[\"label\"]==0).sum()}\")\n",
    "\n",
    "print(\"# dev.tsv\")\n",
    "print(f\"Positive: {(dev_df[\"label\"]==1).sum()}\")\n",
    "print(f\"Negative: {(dev_df[\"label\"]==0).sum()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# train.tsv\n",
    "Positive: 37569\n",
    "Negative: 29780\n",
    "# dev.tsv\n",
    "Positive: 444\n",
    "Negative: 428\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 61. 特徴ベクトル\n",
    "\n",
    "Bag of Words (BoW) に基づき, 学習データ (train.tsv) および検証データ (dev.tsv) のテキストを特徴ベクトルに変換したい. ここで, ある事例のテキストの特徴ベクトルは, テキスト中に含まれる単語 (スペース区切りのトークン) の出現頻度で構成する. 例えば, \"too loud , too goofy\"というテキストに対応する特徴ベクトルは, 以下のような辞書オブジェクトで表現される."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'too': 2, 'loud': 1, ',': 1, 'goofy': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各事例はテキスト, 特徴ベクトル, ラベルを格納した辞書オブジェクトでまとめておく. 例えば, 先ほどの\"too loud , too goofy\"に対してラベル\"0\" (ネガティブ) が付与された事例は, 以下のオブジェクトで表現される."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'text': 'too loud , too goofy', 'label': '0', 'feature': {'too': 2, 'loud': 1, ',': 1, 'goofy': 1}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df   = pd.read_table(\"SST-2/dev.tsv\",   delimiter=\"\\t\")\n",
    "\n",
    "def make_bow_dict_list(df, text_col=\"sentence\", label_col=\"label\"):\n",
    "    result = []\n",
    "    for text, label in zip(df[text_col], df[label_col]):\n",
    "        if not isinstance(text, str) or text.strip()==\"\":\n",
    "            continue\n",
    "        tokens = text.split()          \n",
    "        feat   = dict(Counter(tokens))  \n",
    "        result.append({\n",
    "            \"text\":    text,\n",
    "            \"label\":   str(label),    \n",
    "            \"feature\": feat\n",
    "        })\n",
    "    return result\n",
    "\n",
    "train_list = make_bow_dict_list(train_df)\n",
    "dev_list   = make_bow_dict_list(dev_df)\n",
    "\n",
    "print(train_list[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "{'text': 'hide new secretions from the parental units ', 'label': '0', 'feature': {'hide': 1, 'new': 1, 'secretions': 1, 'from': 1, 'the': 1, 'parental': 1, 'units': 1}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 62. 学習\n",
    "\n",
    "61で構築した学習データの特徴ベクトルを用いて, ロジスティック回帰モデルを学習せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def make_bow_dict_list(df, text_col=\"sentence\", label_col=\"label\"):\n",
    "    data = []\n",
    "    for text, label in zip(df[text_col], df[label_col]):\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "        tokens = text.split()            \n",
    "        feat   = dict(Counter(tokens)) \n",
    "        data.append({\n",
    "            \"text\":    text,\n",
    "            \"label\":   str(label),\n",
    "            \"feature\": feat\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "train_list = make_bow_dict_list(train_df)\n",
    "dev_list   = make_bow_dict_list(dev_df)\n",
    "\n",
    "vocab = set()\n",
    "for entry in train_list:\n",
    "    vocab.update(entry[\"feature\"].keys())\n",
    "vocab_list = sorted(vocab)\n",
    "word2idx = {w:i for i,w in enumerate(vocab_list)}\n",
    "\n",
    "def dict_list_to_matrix(dict_list, word2idx):\n",
    "    X = np.zeros((len(dict_list), len(word2idx)), dtype=int)\n",
    "    y = []\n",
    "    for i, entry in enumerate(dict_list):\n",
    "        for w, cnt in entry[\"feature\"].items():\n",
    "            if w in word2idx:\n",
    "                X[i, word2idx[w]] = cnt\n",
    "        y.append(int(entry[\"label\"]))\n",
    "    return X, np.array(y)\n",
    "\n",
    "X_train, y_train = dict_list_to_matrix(train_list, word2idx)\n",
    "X_dev, y_dev     = dict_list_to_matrix(dev_list,   word2idx)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_dev)\n",
    "acc = accuracy_score(y_dev, y_pred)\n",
    "print(f\"Validation Accuracy: {acc}\")\n",
    "\n",
    "joblib.dump(model, \"logistic_model.pkl\")\n",
    "joblib.dump(word2idx, \"vocab.pkl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Validation Accuracy: 0.8107798165137615\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 63. 予測\n",
    "\n",
    "\n",
    "学習したロジスティック回帰モデルを用い, 検証データの先頭の事例のラベル (ポジネガ) を予測せよ.また, 予測されたラベルが検証データで付与されていたラベルと一致しているか, 確認せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "text = dev_df[\"sentence\"][0]\n",
    "label = dev_df[\"label\"][0]\n",
    "tokens = text.split()\n",
    "feature = dict(Counter(tokens))\n",
    "\n",
    "X = np.zeros((1, len(word2idx)), dtype=int)\n",
    "for w, cnt in feature.items():\n",
    "    if w in word2idx:\n",
    "        X[0, word2idx[w]] = cnt\n",
    "\n",
    "pred = model.predict(X)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"True: {label}\")\n",
    "print(f\"Pred: {pred[0]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Text: it 's a charming and often affecting journey .\n",
    "True: 1\n",
    "Pred: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64. 条件付き確率\n",
    "\n",
    "\n",
    "学習したロジスティック回帰モデルを用い, 検証データの先頭の事例を各ラベル (ポジネガ) に分類するときの条件付き確率を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "text = dev_df[\"sentence\"][0]\n",
    "label = dev_df[\"label\"][0]\n",
    "tokens = text.split()\n",
    "feature = dict(Counter(tokens))\n",
    "\n",
    "X = np.zeros((1, len(word2idx)), dtype=int)\n",
    "for w, cnt in feature.items():\n",
    "    if w in word2idx:\n",
    "        X[0, word2idx[w]] = cnt\n",
    "\n",
    "pred = model.predict(X)\n",
    "proba = model.predict_proba(X)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"True: {label}\")\n",
    "print(f\"Pred: {pred[0]}\")\n",
    "print(f\"Class Prob: {proba[0]}\")\n",
    "print(f\"→ P(nega=0): {proba[0][0]:.4f}, P(posi=1): {proba[0][1]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Text: it 's a charming and often affecting journey .\n",
    "True: 1\n",
    "Pred: 1\n",
    "Class Prob: [0.0042542 0.9957458]\n",
    "→ P(nega=0): 0.0043, P(posi=1): 0.9957457983179792\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 65. テキストのポジネガの予測\n",
    "\n",
    "与えられたテキストのポジネガを予測するプログラムを実装せよ. 例えば、テキストとして\"the worst movie I 've ever seen\"を与え, ロジスティック回帰モデルの予測結果を確認せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "def sentimentAnalyser(text, model, word2idx):\n",
    "    tokens = text.split()\n",
    "    feature = dict(Counter(tokens))\n",
    "    \n",
    "    X = np.zeros((1, len(word2idx)), dtype=int)\n",
    "    for w, cnt in feature.items():\n",
    "        if w in word2idx:\n",
    "            X[0, word2idx[w]] = cnt\n",
    "\n",
    "    pred = model.predict(X)\n",
    "    senti = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    \n",
    "    return senti\n",
    "\n",
    "text = \"the worst movie I 've ever seen\"\n",
    "senti = sentimentAnalyser(text, model, word2idx)\n",
    "\n",
    "print(f\"\\\"{text}\\\" is {senti}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\"the worst movie I 've ever seen\" is Negative.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66. 混同行列の作成\n",
    "\n",
    "学習したロジスティック回帰モデルの検証データにおける混同行列 (confusion matrix) を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "texts = dev_df[\"sentence\"]\n",
    "labels = dev_df[\"label\"]\n",
    "\n",
    "X = np.zeros((len(texts), len(word2idx)), dtype=int)\n",
    "for i, text in enumerate(texts):\n",
    "    tokens = text.split()\n",
    "    feature = dict(Counter(tokens))\n",
    "    for w, cnt in feature.items():\n",
    "        if w in word2idx:\n",
    "            X[i, word2idx[w]] = cnt\n",
    "\n",
    "y_true = labels.to_numpy()\n",
    "y_pred = model.predict(X)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix on Dev Set\")\n",
    "plt.show()\n",
    "plt.savefig(\"c.png\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c](../../_images/c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 67. 精度の計測\n",
    "\n",
    "学習したロジスティック回帰モデルの正解率, 適合率, 再現率, F1スコアを, 学習データおよび検証データ上で計測せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "def make_features(df, word2idx):\n",
    "    texts = df[\"sentence\"]\n",
    "    labels = df[\"label\"]\n",
    "    X = np.zeros((len(texts), len(word2idx)), dtype=int)\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = text.split()\n",
    "        feature = dict(Counter(tokens))\n",
    "        for w, cnt in feature.items():\n",
    "            if w in word2idx:\n",
    "                X[i, word2idx[w]] = cnt\n",
    "    return X, labels.to_numpy()\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df   = pd.read_table(\"SST-2/dev.tsv\",   delimiter=\"\\t\")\n",
    "\n",
    "X_train, y_train = make_features(train_df, word2idx)\n",
    "X_dev, y_dev     = make_features(dev_df, word2idx)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_dev_pred   = model.predict(X_dev)\n",
    "\n",
    "def print_metrics(y_true, y_pred, name=\"\"):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall   : {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score : {f1_score(y_true, y_pred):.4f}\\n\")\n",
    "    \n",
    "print_metrics(y_train, y_train_pred, name=\"Train\")\n",
    "print_metrics(y_dev,   y_dev_pred,   name=\"Dev\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "=== Train ===\n",
    "Accuracy : 0.9422\n",
    "Precision: 0.9428\n",
    "Recall   : 0.9544\n",
    "F1 Score : 0.9485\n",
    "\n",
    "=== Dev ===\n",
    "Accuracy : 0.8108\n",
    "Precision: 0.8000\n",
    "Recall   : 0.8378\n",
    "F1 Score : 0.8185\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 68. 特徴量の重みの確認\n",
    "\n",
    "\n",
    "学習したロジスティック回帰モデルの中で, 重みの高い特徴量トップ20と, 重みの低い特徴量トップ20を確認せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "word2idx = joblib.load(\"vocab.pkl\")\n",
    "\n",
    "coefs = model.coef_[0]\n",
    "\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "top_pos_indices = np.argsort(coefs)[-20:][::-1]\n",
    "top_pos_words = [(idx2word[i], coefs[i]) for i in top_pos_indices]\n",
    "\n",
    "top_neg_indices = np.argsort(coefs)[:20]\n",
    "top_neg_words = [(idx2word[i], coefs[i]) for i in top_neg_indices]\n",
    "\n",
    "print(\"=== Top 20 Positive Words ===\")\n",
    "for word, weight in top_pos_words:\n",
    "    print(f\"{word:<15} {weight:.4f}\")\n",
    "\n",
    "print(\"\\n=== Top 20 Negative Words ===\")\n",
    "for word, weight in top_neg_words:\n",
    "    print(f\"{word:<15} {weight:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "refreshing      3.4152\n",
    "remarkable      3.4090\n",
    "powerful        3.2028\n",
    "hilarious       3.1706\n",
    "beautiful       3.0031\n",
    "wonderful       2.9567\n",
    "prose           2.9239\n",
    "appealing       2.8553\n",
    "terrific        2.8447\n",
    "treat           2.7826\n",
    "enjoyable       2.7641\n",
    "charmer         2.7494\n",
    "vividly         2.7119\n",
    "likable         2.6818\n",
    "solid           2.6587\n",
    "charming        2.6485\n",
    "half-bad        2.6182\n",
    "fascinating     2.6124\n",
    "impressive      2.5906\n",
    "intriguing      2.5776\n",
    "\n",
    "=== Top 20 Negative Words ===\n",
    "lacking         -4.3342\n",
    "lacks           -4.0458\n",
    "worst           -3.9947\n",
    "devoid          -3.6436\n",
    "mess            -3.5915\n",
    "failure         -3.5565\n",
    "stupid          -3.3355\n",
    "bore            -3.2426\n",
    "flat            -3.2277\n",
    "depressing      -3.1854\n",
    "loses           -3.1598\n",
    "waste           -3.1365\n",
    "hardly          -3.0391\n",
    "lack            -3.0281\n",
    "squanders       -3.0261\n",
    "none            -3.0259\n",
    "poor            -2.9788\n",
    "pointless       -2.9419\n",
    "unfortunately   -2.9370\n",
    "lousy           -2.9102\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 69. 正則化パラメータの変更\n",
    "\n",
    "\n",
    "ロジスティック回帰モデルを学習するとき, 正則化の係数 (ハイパーパラメータ) を調整することで, 学習時の適合度合いを制御できる.正則化の係数を変化させながらロジスティック回帰モデルを学習し, 検証データ上の正解率を求めよ.実験の結果は, 正則化パラメータを横軸, 正解率を縦軸としたグラフにまとめよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def make_bow_dict_list(df, text_col=\"sentence\", label_col=\"label\"):\n",
    "    data = []\n",
    "    for text, label in zip(df[text_col], df[label_col]):\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "        tokens = text.split()\n",
    "        feat = dict(Counter(tokens))\n",
    "        data.append({\"text\": text, \"label\": str(label), \"feature\": feat})\n",
    "    return data\n",
    "\n",
    "def dict_list_to_matrix(dict_list, word2idx):\n",
    "    X = np.zeros((len(dict_list), len(word2idx)), dtype=int)\n",
    "    y = []\n",
    "    for i, entry in enumerate(dict_list):\n",
    "        for w, cnt in entry[\"feature\"].items():\n",
    "            if w in word2idx:\n",
    "                X[i, word2idx[w]] = cnt\n",
    "        y.append(int(entry[\"label\"]))\n",
    "    return X, np.array(y)\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "train_list = make_bow_dict_list(train_df)\n",
    "dev_list = make_bow_dict_list(dev_df)\n",
    "\n",
    "vocab = set()\n",
    "for entry in train_list:\n",
    "    vocab.update(entry[\"feature\"].keys())\n",
    "vocab_list = sorted(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab_list)}\n",
    "\n",
    "X_train, y_train = dict_list_to_matrix(train_list, word2idx)\n",
    "X_dev, y_dev = dict_list_to_matrix(dev_list, word2idx)\n",
    "\n",
    "C_values = np.logspace(-3, 2, 10)\n",
    "accuracies = []\n",
    "\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_dev)\n",
    "    acc = accuracy_score(y_dev, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"C={C:.4f}, Accuracy={acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(C_values, accuracies, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Regularization Parameter C (log scale)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Effect of Regularization on Logistic Regression Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"cm.png\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c](../../_images/cm.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}