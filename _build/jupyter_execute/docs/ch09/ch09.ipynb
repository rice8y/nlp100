{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: 事前学習済み言語モデル (BERT型)\n",
    "\n",
    "本章では, BERT型の事前学習済みモデルを利用して, マスク単語の予測や文ベクトルの計算, 評判分析器 (ポジネガ分類器) の構築に取り組む.\n",
    "\n",
    "```{warning}\n",
    "本章は, `code-cell` ではなく, Markdown のコードブロック内にコードを記述しているため, Google Colab上で直接実行できません.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. トークン化\n",
    "\n",
    "\"The movie was full of incomprehensibilities.\"という文をトークンに分解し, トークン列を表示せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"The movie was full of incomprehensibilities.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. マスクの予測\n",
    "\n",
    "\"The movie was full of [MASK].\"の\"[MASK]\"を埋めるのに最も適切なトークンを求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "unmasker = pipeline(\"fill-mask\", model=model_name)\n",
    "\n",
    "masked_text = \"The movie was full of [MASK].\"\n",
    "results = unmasker(masked_text)\n",
    "\n",
    "print(f'[MASK]: {results[0][\"token_str\"]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[MASK]: fun\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. マスクのtop-k予測\n",
    "\n",
    "\"The movie was full of [MASK].\"の\"[MASK]\"に埋めるのに適切なトークン上位10個と, その確率 (尤度) を求めよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "unmasker = pipeline(\"fill-mask\", model=model_name)\n",
    "\n",
    "masked_text = \"The movie was full of [MASK].\"\n",
    "results = unmasker(masked_text, top_k=10)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'[MASK]: {results[i][\"token_str\"]} (score: {results[i][\"score\"]})')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[MASK]: fun (score: 0.10711898654699326)\n",
    "[MASK]: surprises (score: 0.06634481996297836)\n",
    "[MASK]: drama (score: 0.04468412697315216)\n",
    "[MASK]: stars (score: 0.027217047289013863)\n",
    "[MASK]: laughs (score: 0.02541276253759861)\n",
    "[MASK]: action (score: 0.019516924396157265)\n",
    "[MASK]: excitement (score: 0.019038109108805656)\n",
    "[MASK]: people (score: 0.018290270119905472)\n",
    "[MASK]: tension (score: 0.015030566602945328)\n",
    "[MASK]: music (score: 0.014646219089627266)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. CLSトークンによる文ベクトル\n",
    "\n",
    "以下の文の全ての組み合わせに対して, 最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ.\n",
    "\n",
    "- \"The movie was full of fun.\"\n",
    "- \"The movie was full of excitement.\"\n",
    "- \"The movie was full of crap.\"\n",
    "- \"The movie was full of rubbish.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = [\n",
    "    \"The movie was full of fun.\",\n",
    "    \"The movie was full of excitement.\",\n",
    "    \"The movie was full of crap.\",\n",
    "    \"The movie was full of rubbish.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "cls_embs = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "cls_np = cls_embs.cpu().numpy()\n",
    "cos_sim_mat = cosine_similarity(cls_np)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cos_sim_mat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Cosine Similarity Matrix:\n",
    "[[0.99999976 0.98806083 0.95576596 0.9475324 ]\n",
    " [0.98806083 1.         0.95412743 0.9486636 ]\n",
    " [0.95576596 0.95412743 0.9999998  0.9806931 ]\n",
    " [0.9475324  0.9486636  0.9806931  1.0000002 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 平均による文ベクトル\n",
    "\n",
    "以下の文の全ての組み合わせに対して, 最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ.\n",
    "\n",
    "- \"The movie was full of fun.\"\n",
    "- \"The movie was full of excitement.\"\n",
    "- \"The movie was full of crap.\"\n",
    "- \"The movie was full of rubbish.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = [\n",
    "    \"The movie was full of fun.\",\n",
    "    \"The movie was full of excitement.\",\n",
    "    \"The movie was full of crap.\",\n",
    "    \"The movie was full of rubbish.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "last_hidden = outputs.last_hidden_state\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "masked_hidden = last_hidden * mask\n",
    "sum_hidden = masked_hidden.sum(1)\n",
    "sum_mask = mask.sum(1)\n",
    "mean_pooled = sum_hidden / sum_mask\n",
    "\n",
    "mean_vecs = mean_pooled.cpu().numpy()\n",
    "cos_sim_mat = cosine_similarity(mean_vecs)\n",
    "cos_sim_mat = np.clip(cos_sim_mat, -1.0, 1.0)\n",
    "\n",
    "print(\"Cosine Similarity Matrix (Mean Pooling):\")\n",
    "print(cos_sim_mat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Cosine Similarity Matrix (Mean Pooling):\n",
    "[[1.         0.95681167 0.8489993  0.81688446]\n",
    " [0.95681167 0.99999976 0.83518374 0.79384434]\n",
    " [0.8489993  0.83518374 1.         0.9225539 ]\n",
    " [0.81688446 0.79384434 0.9225539  0.9999999 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. データセットの準備\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) から訓練セット (train.tsv) と開発セット (dev.tsv) のテキストと極性ラベルと読み込み, さらに全てのテキストはトークン列に変換せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "train_df = pd.read_table(\"../ch07/SST-2/train.tsv\", delimiter='\\t')\n",
    "dev_df = pd.read_table(\"../ch07/SST-2/dev.tsv\", delimiter='\\t')\n",
    "\n",
    "def tokenize_all_text(df, tokenizer):\n",
    "    df[\"tokens\"] = [None] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        text = df[\"sentence\"][i]\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        df[\"tokens\"][i] = tokens\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_df = tokenize_all_text(train_df, tokenizer)\n",
    "dev_df = tokenize_all_text(dev_df, tokenizer)\n",
    "\n",
    "train_df.to_csv(\"SST-2/train.tsv\", index=False, sep='\\t')\n",
    "dev_df.to_csv(\"SST-2/dev.tsv\", index=False, sep='\\t')\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_df.head())\n",
    "print(\"Dev:\")\n",
    "print(dev_df.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Train:\n",
    "                                            sentence  label                                             tokens\n",
    "0       hide new secretions from the parental units       0  [hide, new, secret, ##ions, from, the, parenta...\n",
    "1               contains no wit , only labored gags       0  [contains, no, wit, ,, only, labor, ##ed, gag,...\n",
    "2  that loves its characters and communicates som...      1  [that, loves, its, characters, and, communicat...\n",
    "3  remains utterly satisfied to remain the same t...      0  [remains, utterly, satisfied, to, remain, the,...\n",
    "4  on the worst revenge-of-the-nerds clichés the ...      0  [on, the, worst, revenge, -, of, -, the, -, ne...\n",
    "Dev:\n",
    "                                            sentence  label                                             tokens\n",
    "0    it 's a charming and often affecting journey .       1  [it, ', s, a, charming, and, often, affecting,...\n",
    "1                 unflinchingly bleak and desperate       0  [un, ##fl, ##in, ##ching, ##ly, bleak, and, de...\n",
    "2  allows us to hope that nolan is poised to emba...      1  [allows, us, to, hope, that, nolan, is, poised...\n",
    "3  the acting , costumes , music , cinematography...      1  [the, acting, ,, costumes, ,, music, ,, cinema...\n",
    "4                  it 's slow -- very , very slow .       0     [it, ', s, slow, -, -, very, ,, very, slow, .]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. ミニバッチの作成\n",
    "\n",
    "85で読み込んだ訓練データの一部 (例えば冒頭の4事例) に対して, パディングなどの処理を行い, トークン列の長さを揃えてミニバッチを構成せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "train_df = pd.read_table(\"../ch07/SST-2/train.tsv\", delimiter='\\t')\n",
    "dev_df = pd.read_table(\"../ch07/SST-2/dev.tsv\", delimiter='\\t')\n",
    "\n",
    "def tokenize_all_text(df, tokenizer):\n",
    "    df[\"tokens\"] = [None] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        text = df[\"sentence\"][i]\n",
    "        tokens = tokenizer.tokenize(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        df[\"tokens\"][i] = tokens\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_df = tokenize_all_text(train_df[:4], tokenizer)\n",
    "dev_df = tokenize_all_text(dev_df[:4], tokenizer)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_df.head())\n",
    "print(\"Dev:\")\n",
    "print(dev_df.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. ファインチューニング\n",
    "\n",
    "訓練セットを用い, 事前学習済みモデルを極性分析タスク向けにファインチューニングせよ.検証セット上でファインチューニングされたモデルの正解率を計測せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tsv2json(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        reader = csv.DictReader(f_in, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            data = {\n",
    "                \"sentence\": row['sentence'].strip(),\n",
    "                \"label\": int(row['label'].strip())\n",
    "            }\n",
    "            f_out.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "tsv2json(\"../ch07/SST-2/train.tsv\", \"SST-2/train.json\")\n",
    "tsv2json(\"../ch07/SST-2/dev.tsv\", \"SST-2/dev.json\")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Fine-tune BERT model for sentiment analysis\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the pre-trained BERT model\")\n",
    "    parser.add_argument(\"--lr\", type=float, required=True, help=\"Learning rate for training\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for regularization (default: 0.01)\")\n",
    "    parser.add_argument(\"--max_epochs\", type=int, required=True, help=\"Maximum number of epochs for training\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def setup_logging():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"log/training_{timestamp}.log\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Logging setup complete. Logs will be saved to: {log_filename}\")\n",
    "    return log_filename\n",
    "\n",
    "def make_dataset(tokenizer, max_length, texts, labels):\n",
    "    dataset_for_loader = list()\n",
    "    for text, label in zip(texts, labels):\n",
    "        encoding = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        encoding[\"labels\"] = label\n",
    "        encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "class SentimentAnalyzer(pl.LightningModule):\n",
    "    def __init__(self, model, num_labels, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.bert_sc(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        acc = accuracy_score(labels.cpu().numpy(), labels_predicted.cpu().numpy())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        acc = accuracy_score(labels.cpu().numpy(), labels_predicted.cpu().numpy())\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "def main():\n",
    "    log_file = setup_logging()\n",
    "    args = parse_args()\n",
    "    logging.info(\"Starting training with arguments:\")\n",
    "    logging.info(vars(args))\n",
    "\n",
    "    train_df = pd.read_json('SST-2/train.json', lines=True)\n",
    "    val_df = pd.read_json('SST-2/dev.json', lines=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    max_length = 100\n",
    "\n",
    "    train = make_dataset(tokenizer, max_length, train_df[\"sentence\"].tolist(), train_df[\"label\"].tolist())\n",
    "    val = make_dataset(tokenizer, max_length, val_df[\"sentence\"].tolist(), val_df[\"label\"].tolist())\n",
    "\n",
    "    dataloader_train = DataLoader(train, batch_size=64, shuffle=True)\n",
    "    dataloader_val = DataLoader(val, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = SentimentAnalyzer(\n",
    "        args.model, num_labels=2, lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_acc\", mode=\"max\", save_top_k=1,\n",
    "        save_weights_only=True, dirpath=\"model/\"\n",
    "    )\n",
    "\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        devices=1,\n",
    "        accelerator=\"auto\",\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    logging.info(\"Trainer initialized. Starting training...\")\n",
    "    trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"./models/{args.model.split('/')[-1]}_lr{args.lr}_wd{args.weight_decay}_epochs{args.max_epochs}_{timestamp}.ckpt\"\n",
    "    trainer.save_checkpoint(model_path)\n",
    "\n",
    "    logging.info(f\"Best model saved at: {checkpoint.best_model_path}\")\n",
    "    logging.info(f\"Best validation ACC: {checkpoint.best_model_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:17<00:00,  2.40it/s, v_num=2]\n",
    "Metric val_acc improved. New best score: 0.817\n",
    "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:25<00:00,  2.36it/s, v_num=2]\n",
    "Metric val_acc improved by 0.010 >= min_delta = 0.0. New best score: 0.827\n",
    "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:25<00:00,  2.36it/s, v_num=2]\n",
    "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
    "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:25<00:00,  2.36it/s, v_num=2]\n",
    "2025-04-16 11:35:05,228 - INFO - Best model saved at: /net/nas8/data/home/yoneyama/workspace/nlp100_2025/ch09/model/epoch=1-step=2106.ckpt\n",
    "2025-04-16 11:35:05,244 - INFO - Best validation ACC: 0.8268348574638367\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 極性分析\n",
    "\n",
    "問題87でファインチューニングされたモデルを用いて, 以下の文の極性を予測せよ.\n",
    "\n",
    "- \"The movie was full of incomprehensibilities.\"\n",
    "- \"The movie was full of fun.\"\n",
    "- \"The movie was full of excitement.\"\n",
    "- \"The movie was full of crap.\"\n",
    "- \"The movie was full of rubbish.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Fine-tune BERT model for sentiment analysis\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the pre-trained BERT model\")\n",
    "    parser.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to the .ckpt\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "class SentimentAnalyzer(pl.LightningModule):\n",
    "    def __init__(self, model, num_labels, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.bert_sc(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        acc = accuracy_score(labels.cpu().numpy(), labels_predicted.cpu().numpy())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        acc = accuracy_score(labels.cpu().numpy(), labels_predicted.cpu().numpy())\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "def inference(text, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert_sc(**tokens)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        pred_class = torch.argmax(probs, dim=-1).item()\n",
    "    return pred_class\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    model = SentimentAnalyzer.load_from_checkpoint(args.ckpt)\n",
    "    \n",
    "    example = [\n",
    "        \"The movie was full of incomprehensibilities.\",\n",
    "        \"The movie was full of fun.\",\n",
    "        \"The movie was full of excitement.\",\n",
    "        \"The movie was full of crap.\",\n",
    "        \"The movie was full of rubbish.\"\n",
    "    ]\n",
    "    \n",
    "    for text in example:\n",
    "        pred = inference(text, tokenizer, model)\n",
    "        print(f\"{text} -> {pred}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "The movie was full of incomprehensibilities. -> 0\n",
    "The movie was full of fun. -> 1\n",
    "The movie was full of excitement. -> 1\n",
    "The movie was full of crap. -> 0\n",
    "The movie was full of rubbish. -> 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. アーキテクチャの変更\n",
    "\n",
    "問題87とは異なるアーキテクチャ (例えば[CLS]トークンを用いるか, 各トークンの最大値プーリングを用いるなど) の分類モデルを設計し, 事前学習済みモデルを極性分析タスク向けにファインチューニングせよ.検証セット上でファインチューニングされたモデルの正解率を計測せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tsv2json(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        reader = csv.DictReader(f_in, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            data = {\n",
    "                \"sentence\": row['sentence'].strip(),\n",
    "                \"label\": int(row['label'].strip())\n",
    "            }\n",
    "            f_out.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "tsv2json(\"../ch07/SST-2/train.tsv\", \"SST-2/train.json\")\n",
    "tsv2json(\"../ch07/SST-2/dev.tsv\", \"SST-2/dev.json\")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Fine-tune BERT model for sentiment analysis\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the pre-trained BERT model\")\n",
    "    parser.add_argument(\"--lr\", type=float, required=True, help=\"Learning rate for training\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for regularization (default: 0.01)\")\n",
    "    parser.add_argument(\"--max_epochs\", type=int, required=True, help=\"Maximum number of epochs for training\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def setup_logging():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"log/training_{timestamp}.log\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Logging setup complete. Logs will be saved to: {log_filename}\")\n",
    "    return log_filename\n",
    "\n",
    "def make_dataset(tokenizer, max_length, texts, labels):\n",
    "    dataset_for_loader = list()\n",
    "    for text, label in zip(texts, labels):\n",
    "        encoding = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        encoding[\"labels\"] = label\n",
    "        encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "class SentimentAnalyzer(pl.LightningModule):\n",
    "    def __init__(self, model, num_labels, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled = torch.max(last_hidden_state, dim=1).values\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self.forward(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self.forward(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        val_loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "    \n",
    "def main():\n",
    "    log_file = setup_logging()\n",
    "    args = parse_args()\n",
    "    logging.info(\"Starting training with arguments:\")\n",
    "    logging.info(vars(args))\n",
    "\n",
    "    train_df = pd.read_json('SST-2/train.json', lines=True)\n",
    "    val_df = pd.read_json('SST-2/dev.json', lines=True)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    max_length = 100\n",
    "\n",
    "    train = make_dataset(tokenizer, max_length, train_df[\"sentence\"].tolist(), train_df[\"label\"].tolist())\n",
    "    val = make_dataset(tokenizer, max_length, val_df[\"sentence\"].tolist(), val_df[\"label\"].tolist())\n",
    "\n",
    "    dataloader_train = DataLoader(train, batch_size=64, shuffle=True)\n",
    "    dataloader_val = DataLoader(val, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = SentimentAnalyzer(\n",
    "        args.model, num_labels=2, lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_acc\", mode=\"max\", save_top_k=1,\n",
    "        save_weights_only=True, dirpath=\"model/\"\n",
    "    )\n",
    "\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        devices=1,\n",
    "        accelerator=\"auto\",\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    logging.info(\"Trainer initialized. Starting training...\")\n",
    "    trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"./models/{args.model.split('/')[-1]}_lr{args.lr}_wd{args.weight_decay}_epochs{args.max_epochs}_{timestamp}.ckpt\"\n",
    "    trainer.save_checkpoint(model_path)\n",
    "\n",
    "    logging.info(f\"Best model saved at: {checkpoint.best_model_path}\")\n",
    "    logging.info(f\"Best validation ACC: {checkpoint.best_model_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:30<00:00,  2.34it/s, v_num=3]\n",
    "Metric val_acc improved. New best score: 0.794\n",
    "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:38<00:00,  2.30it/s, v_num=3]\n",
    "Metric val_acc improved by 0.022 >= min_delta = 0.0. New best score: 0.815\n",
    "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:36<00:00,  2.31it/s, v_num=3]\n",
    "Metric val_acc improved by 0.011 >= min_delta = 0.0. New best score: 0.827\n",
    "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
    "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1053/1053 [07:43<00:00,  2.27it/s, v_num=3]\n",
    "2025-04-16 12:10:02,356 - INFO - Best model saved at: /net/nas8/data/home/yoneyama/workspace/nlp100_2025/ch09/model/epoch=2-step=3159.ckpt\n",
    "2025-04-16 12:10:02,376 - INFO - Best validation ACC: 0.8268348574638367\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}