{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語埋め込みの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = model[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "print(f\"#train: {len(train_data)}, #dev: {len(dev_data)}\")\n",
    "print(\"Example: \", train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. Bag of wordsモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeanEmmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        mask = (input_ids != 0).unsqueeze(-1)\n",
    "        masked_embed = embedded * mask\n",
    "        \n",
    "        sum_embed = masked_embed.sum(dim=1)\n",
    "        cnt = mask.sum(dim=1).clamp(min=1)\n",
    "        mean_embed = sum_embed / cnt\n",
    "        \n",
    "        logits = self.linear(mean_embed).squeeze(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. モデルの学習 - 77. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class MeanEmmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        mask = (input_ids != 0).unsqueeze(-1)\n",
    "        masked_embed = embedded * mask\n",
    "        \n",
    "        sum_embed = masked_embed.sum(dim=1)\n",
    "        cnt = mask.sum(dim=1).clamp(min=1)\n",
    "        mean_embed = sum_embed / cnt\n",
    "        \n",
    "        logits = self.linear(mean_embed).squeeze(1)\n",
    "        return logits\n",
    "    \n",
    "def collate(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = torch.cat([item[\"label\"] for item in batch])\n",
    "    \n",
    "    padded_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    return {\"input_ids\": padded_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MeanEmmbeddingClassifier(embedding_matrix).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_loader, dev_loader):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            dev_batch_loss.append(loss.item())\n",
    "\n",
    "    train_acc = eval_model(model, train_loader)\n",
    "    dev_acc = eval_model(model, dev_loader)\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(dev_batch_loss), train_acc, dev_acc\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "epoch = 100\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    model, train_l, dev_l, train_a, dev_a = train_model(model, train_loader, dev_loader)\n",
    "    train_loss.append(train_l)\n",
    "    dev_loss.append(dev_l)\n",
    "    train_acc.append(train_a)\n",
    "    dev_acc.append(dev_a)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}]\")\n",
    "        print(f\"Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "        print(f\"Train acc : {train_a:.4f}, Dev acc : {dev_a:.4f}\")\n",
    "        \n",
    "path_saved_model = \"./models/model_ex73.pt\"\n",
    "torch.save(model.state_dict(), path_saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. 単語埋め込みのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../ch06/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "vocab_size = len(w2v.key_to_index) + 1\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "for idx, word in enumerate(w2v.key_to_index, start=1):\n",
    "    embedding_matrix[idx] = w2v[word]\n",
    "    token2id[word] = idx\n",
    "    id2token[idx] = word\n",
    "    \n",
    "def load_sst(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    examples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"sentence\"]\n",
    "        label = float(row[\"label\"])\n",
    "        tokens = text.split()\n",
    "        input_ids = [token2id[t] for t in tokens if t in token2id]\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "        examples.append({\n",
    "            \"text\": text,\n",
    "            \"label\": torch.tensor([label], dtype=torch.float32),\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "train_data = load_sst(\"../ch07/SST-2/train.tsv\")\n",
    "dev_data   = load_sst(\"../ch07/SST-2/dev.tsv\")\n",
    "\n",
    "class MeanEmmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        mask = (input_ids != 0).unsqueeze(-1)\n",
    "        masked_embed = embedded * mask\n",
    "        \n",
    "        sum_embed = masked_embed.sum(dim=1)\n",
    "        cnt = mask.sum(dim=1).clamp(min=1)\n",
    "        mean_embed = sum_embed / cnt\n",
    "        \n",
    "        logits = self.linear(mean_embed).squeeze(1)\n",
    "        return logits\n",
    "    \n",
    "def collate(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = torch.cat([item[\"label\"] for item in batch])\n",
    "    \n",
    "    padded_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    return {\"input_ids\": padded_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MeanEmmbeddingClassifier(embedding_matrix, freeze_embedding=False).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_model(model, train_loader, dev_loader):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            dev_batch_loss.append(loss.item())\n",
    "\n",
    "    train_acc = eval_model(model, train_loader)\n",
    "    dev_acc = eval_model(model, dev_loader)\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(dev_batch_loss), train_acc, dev_acc\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "epoch = 100\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    model, train_l, dev_l, train_a, dev_a = train_model(model, train_loader, dev_loader)\n",
    "    train_loss.append(train_l)\n",
    "    dev_loss.append(dev_l)\n",
    "    train_acc.append(train_a)\n",
    "    dev_acc.append(dev_a)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}]\")\n",
    "        print(f\"Train loss: {train_l:.4f}, Dev loss: {dev_l:.4f}\")\n",
    "        print(f\"Train acc : {train_a:.4f}, Dev acc : {dev_a:.4f}\")\n",
    "        \n",
    "path_saved_model = \"./models/model_ex78.pt\"\n",
    "torch.save(model.state_dict(), path_saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. アーキテクチャの変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
