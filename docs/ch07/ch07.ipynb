{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章: 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では, [Stanford Sentiment Treebank (SST)](https://nlp.stanford.edu/sentiment/) データセットを用い, 評判分析器 (ポジネガ分類器) を構築する. ここでは処理を簡略化するため, [General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されているSSTデータセットを用いる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. データの入手・整形\n",
    "\n",
    "GLUEのウェブサイトから[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)データセットを取得せよ。学習データ（`train.tsv`）と検証データ（`dev.tsv`）のぞれぞれについて、ポジティブ (1) とネガティブ (0) の事例数をカウントせよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-10 17:12:36--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘dl.fbaipublicfiles.com’\n",
      "unzip:  cannot find or open https://dl.fbaipublicfiles.com/glue/data/SST-2.zip, https://dl.fbaipublicfiles.com/glue/data/SST-2.zip.zip or https://dl.fbaipublicfiles.com/glue/data/SST-2.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
    "# !unzip SST-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(\"# train.tsv\")\n",
    "print(f\"Positive: {(train_df[\"label\"]==1).sum()}\")\n",
    "print(f\"Negative: {(train_df[\"label\"]==0).sum()}\")\n",
    "\n",
    "print(\"# dev.tsv\")\n",
    "print(f\"Positive: {(dev_df[\"label\"]==1).sum()}\")\n",
    "print(f\"Negative: {(dev_df[\"label\"]==0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 61. 特徴ベクトル\n",
    "\n",
    "Bag of Words (BoW) に基づき, 学習データ (train.tsv) および検証データ (dev.tsv) のテキストを特徴ベクトルに変換したい. ここで, ある事例のテキストの特徴ベクトルは, テキスト中に含まれる単語 (スペース区切りのトークン) の出現頻度で構成する. 例えば, \"too loud , too goofy\"というテキストに対応する特徴ベクトルは, 以下のような辞書オブジェクトで表現される."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'too': 2, 'loud': 1, ',': 1, 'goofy': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各事例はテキスト, 特徴ベクトル, ラベルを格納した辞書オブジェクトでまとめておく. 例えば, 先ほどの\"too loud , too goofy\"に対してラベル\"0\" (ネガティブ) が付与された事例は, 以下のオブジェクトで表現される."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'text': 'too loud , too goofy', 'label': '0', 'feature': {'too': 2, 'loud': 1, ',': 1, 'goofy': 1}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_BoW_dict(df):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        text = df[\"sentence\"][i]\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=None)\n",
    "            X = vectorizer.fit_transform([text])\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "            arr = X.toarray()[0]\n",
    "\n",
    "            feature_dict = {word: arr[idx] for idx, word in enumerate(vocab)}\n",
    "            \n",
    "            bow_dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"feature\": feature_dict\n",
    "            }\n",
    "            data.append(bow_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping row {i} due to error: {e}\")\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_feature_vectors(bow_list):\n",
    "    vocab_set = set()\n",
    "    for item in bow_list:\n",
    "        vocab_set.update(item[\"feature\"].keys())\n",
    "    vocab_list = sorted(vocab_set)\n",
    "\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "    X = np.zeros((len(bow_list), len(vocab_list)))\n",
    "    y = []\n",
    "\n",
    "    for i, item in enumerate(bow_list):\n",
    "        for word, count in item[\"feature\"].items():\n",
    "            idx = word2idx[word]\n",
    "            X[i, idx] = count\n",
    "        y.append(item[\"label\"])\n",
    "    \n",
    "    return X, y, vocab_list\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "train_dict = create_BoW_dict(train_df)\n",
    "dev_dict = create_BoW_dict(dev_df)\n",
    "\n",
    "X_train, y_train, vocab_train = get_feature_vectors(train_dict)\n",
    "X_dev, y_dev, vocab_dev = get_feature_vectors(dev_dict)\n",
    "\n",
    "print(train_df[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_BoW_dict(df):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        text = df[\"sentence\"][i]\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b|[()]\")\n",
    "            X = vectorizer.fit_transform([text])\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "            arr = X.toarray()[0]\n",
    "\n",
    "            feature_dict = {word: arr[idx] for idx, word in enumerate(vocab)}\n",
    "            \n",
    "            bow_dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"feature\": feature_dict\n",
    "            }\n",
    "            data.append(bow_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping row {i} due to error: {e}\")\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_feature_vectors(bow_list, vocab_list=None):\n",
    "    \n",
    "    if vocab_list is None:\n",
    "        vocab_set = set()\n",
    "        for item in bow_list:\n",
    "            vocab_set.update(item[\"feature\"].keys())\n",
    "        vocab_list = sorted(vocab_set)\n",
    "    else:\n",
    "        vocab_list = vocab_list\n",
    "\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "    X = np.zeros((len(bow_list), len(vocab_list)))\n",
    "    y = []\n",
    "\n",
    "    for i, item in enumerate(bow_list):\n",
    "        for word, count in item[\"feature\"].items():\n",
    "            # Skip words not in training vocabulary\n",
    "            if word in word2idx:\n",
    "                idx = word2idx[word]\n",
    "                X[i, idx] = count\n",
    "        y.append(item[\"label\"])\n",
    "    \n",
    "    return X, y, vocab_list\n",
    "\n",
    "train_df = pd.read_table(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "train_dict = create_BoW_dict(train_df)\n",
    "dev_dict = create_BoW_dict(dev_df)\n",
    "\n",
    "X_train, y_train, vocab_train = get_feature_vectors(train_dict)\n",
    "X_dev, y_dev, _ = get_feature_vectors(dev_dict, vocab_train)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_dev)\n",
    "\n",
    "accuracy = accuracy_score(y_dev, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
