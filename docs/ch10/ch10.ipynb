{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章: 事前学習済み言語モデル (GPT型)\n",
    "\n",
    "\n",
    "本章では, GPT型 (Transformerのデコーダ型) の事前学習済みモデルを利用して, 言語生成, 評判分析器 (ポジネガ分類器) の構築, ファインチューニング, 強化学習などに取り組む.\n",
    "\n",
    "```{warning}\n",
    "本章は, `code-cell` ではなく, Markdown のコードブロック内にコードを記述しているため, Google Colab上で直接実行できません.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "**ToDo:** 98, 99は`gpt2-medium`から`HuggingFaceH4/zephyr-7b-beta`に変更する.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. 次単語予測\n",
    "\n",
    "\n",
    "\"The movie was full of\"に続くトークン (トークン列ではなく一つのトークンであることに注意せよ) として適切なもの上位10個と, その確率 (尤度) を求めよ.ただし, 言語モデルへのプロンプトがどのようなトークン列に変換されたか, 確認せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"The movie was full of\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "next_token_logits = logits[0, -1]\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "k = 10\n",
    "top_probs, top_indices = torch.topk(probs, k)\n",
    "\n",
    "for i in range(k):\n",
    "    decoded_text = tokenizer.decode([top_indices[i]])\n",
    "    print(f\"{decoded_text}: {top_probs[i]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    " great: 0.02309427782893181\n",
    " references: 0.013511945493519306\n",
    " action: 0.013043278828263283\n",
    " moments: 0.012449497357010841\n",
    " the: 0.01186001393944025\n",
    " characters: 0.008720042183995247\n",
    " these: 0.007216328755021095\n",
    " surprises: 0.006894332356750965\n",
    " fun: 0.006525978911668062\n",
    " them: 0.006154114380478859\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 続きのテキストの予測\n",
    "\n",
    "\n",
    "\"The movie was full of\"に続くテキストを複数予測せよ.このとき, デコーディングの方法や温度パラメータ (temperature) を変えながら, 予測される複数のテキストの変化を観察せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(1234)\n",
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "\n",
    "text = \"The movie was full of\"\n",
    "\n",
    "for t in [0.2, 0.4, 0.7, 0.9]:\n",
    "    outputs = generator(text, max_length=30, num_return_sequences=1, temperature=t)\n",
    "    print(f'Temp={t}: {outputs[0][\"generated_text\"]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Temp=0.2: The movie was full of great moments, but it was also a bit of a letdown. The film was a bit of a letdown.\n",
    "\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Temp=0.4: The movie was full of action sequences that were very exciting to watch. The movie was very well done, and I think it was a great movie.\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Temp=0.7: The movie was full of all the things I love about this country: the rich, the black, the working class,\" said D'Amato.\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Temp=0.9: The movie was full of action and violence. The scene in the prison in which Lee and Davis are talking and the scene in which Lee kills John,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. 予測されたテキストの確率を計算\n",
    "\n",
    "\"The movie was full of\"に続くテキストを予測し, 生成された各単語の尤度を表示せよ (生成されるテキストが長いと出力が読みにくくなるので, 適当な長さで生成を打ち切るとよい) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. パープレキシティ\n",
    "\n",
    "適当な文を準備して, 事前学習済み言語モデルでパープレキシティを測定せよ.例えば, \n",
    "\n",
    "- The movie was full of surprises\n",
    "- The movies were full of surprises\n",
    "- The movie were full of surprises\n",
    "- The movies was full of surprises\n",
    "\n",
    "\n",
    "の4文に対して, パープレキシティを測定して観察せよ (最後の2つの文は故意に文法的な間違いを入れた)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def calc_ppl(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    return math.exp(loss.mean().item())\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\n",
    "    \"The movie was full of surprises\",\n",
    "    \"The movies were full of surprises\",\n",
    "    \"The movie were full of surprises\",\n",
    "    \"The movies was full of surprises\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f'{sentence}: {calc_ppl(sentence)}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "The movie was full of surprises: 89.45385588749441\n",
    "The movies were full of surprises: 164.8886480442875\n",
    "The movie were full of surprises: 324.1063371099563\n",
    "The movies was full of surprises: 388.4464576262505\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. チャットテンプレート\n",
    "\n",
    "\n",
    "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため, チャットテンプレートを適用し, 言語モデルに与えるべきプロンプトを作成せよ.また, そのプロンプトに対する応答を生成し, 表示せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(1234)    \n",
    "    \n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an excellent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "<|system|>\n",
    "You are an excellent assistant.\n",
    "<|user|>\n",
    "What do you call a sweet eaten after dinner?\n",
    "<|assistant|>\n",
    "A dessert or a dessert item, such as cake, pie, cookies, ice cream, or fruit with a sweet sauce. The specific term may vary based on the type of sweet being referred to.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. マルチターンのチャット\n",
    "\n",
    "\n",
    "問題94で生成された応答に対して, 追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ.また, その時に言語モデルに与えるプロンプトを確認せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(1234)  \n",
    "\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "def extract_assistant_reply(text):\n",
    "    text = text.split(\"<|assistant|>\")\n",
    "    return text[-1].replace(\"\\n\", \"\").strip()\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an excellent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "raw_text = generate_text(prompt)\n",
    "decoded_text = tokenizer.decode(raw_text, skip_special_tokens=True)\n",
    "\n",
    "chat.append({\"role\": \"assistant\", \"content\": extract_assistant_reply(decoded_text)})\n",
    "chat.append({\"role\": \"user\", \"content\": \"Please give me the plural form of the word with its spelling in reverse order.\"})\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "raw_text = generate_text(prompt)\n",
    "decoded_text = tokenizer.decode(raw_text, skip_special_tokens=True)\n",
    "print(decoded_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "<|system|>\n",
    "You are an excellent assistant.\n",
    "<|user|>\n",
    "What do you call a sweet eaten after dinner?\n",
    "<|assistant|>\n",
    "A dessert or a dessert item, such as cake, pie, cookies, ice cream, or fruit with a sweet sauce. The specific term may vary based on the type of sweet being referred to.\n",
    "<|user|>\n",
    "Please give me the plural form of the word with its spelling in reverse order.\n",
    "<|assistant|>\n",
    "If the word ends in \"s,\" simply add an \"es\" to the end. However, if the word ends in \"ch,\" \"sh,\" \"x,\" or \"s\" preceded by a consonant, you add \"es\" and change the \"s\" to \"es\" as well. For example:\n",
    "\n",
    "- Churches (churches)\n",
    "- Boxes (boxes)\n",
    "-Ches (chess)\n",
    "-Kisses (kisses)\n",
    "\n",
    "In reverse order, the plural forms would be:\n",
    "\n",
    "- Esreuq siht noitavT (desserts)\n",
    "- EttorC siht noitavT (cookies)\n",
    "- EhT roE siht noitavT (cakes)\n",
    "- EhT roE siht noitavT (pies)\n",
    "- EhT roE siht noitavT (ice cream)\n",
    "- EhT roE siht noitavT (fruit)\n",
    "- EhT roE siht noitavT (sauces)\n",
    "\n",
    "Note: The reverse spelling is just for fun and is not a commonly used method for spelling words in English.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. プロンプトによる感情分析\n",
    "\n",
    "\n",
    "事前学習済み言語モデルで感情分析を行いたい.テキストを含むプロンプトを事前学習済み言語モデルに与え, (ファインチューニングは行わずに) テキストのポジネガを予測するという戦略で, [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, device, max_new_tokens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "def extract_assistant_reply(text):\n",
    "    text = text.rstrip()\n",
    "    return text[-1]\n",
    "\n",
    "def generate_examples(df, k, random_state=1234):\n",
    "    pos_examples = df[df[\"label\"] == 1].sample(n=k, random_state=random_state)\n",
    "    neg_examples = df[df[\"label\"] == 0].sample(n=k, random_state=random_state)\n",
    "    examples = list(zip(pos_examples[\"sentence\"], pos_examples[\"label\"])) + list(zip(neg_examples[\"sentence\"], neg_examples[\"label\"]))\n",
    "    \n",
    "    prompt = []\n",
    "    for sentence, label in examples:\n",
    "        prompt.append({\"role\": \"user\", \"content\": sentence})\n",
    "        prompt.append({\"role\": \"assistant\", \"content\": str(label)})\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "train_df = pd.read_table(\"../ch07/SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"../ch07/SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokens = tokenizer.tokenize(\"<|assistant|>\\n1\")\n",
    "max_tokens = len(tokens)\n",
    "\n",
    "device = model.device\n",
    "\n",
    "preds = []\n",
    "valid_labels = []\n",
    "skipped_count = 0\n",
    "examples = generate_examples(train_df, k=10)\n",
    "for i in tqdm(range(len(dev_df)), desc=\"Progress\"):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional in emotional analysis. Please judge the following sentences positively or negatively. If it is positive, output 1, if it is negative, output 0.\"},\n",
    "        *examples,\n",
    "        {\"role\": \"user\", \"content\": dev_df[\"sentence\"][i]}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "    raw_output = generate_text(prompt, tokenizer, model, device, max_tokens)\n",
    "    decoded_text = tokenizer.decode(raw_output, skip_special_tokens=True)\n",
    "    pred = extract_assistant_reply(decoded_text)\n",
    "\n",
    "    if pred in {\"0\", \"1\"}:\n",
    "        preds.append(int(pred))\n",
    "        valid_labels.append(int(dev_df[\"label\"][i]))\n",
    "    else:\n",
    "        skipped_count += 1\n",
    "\n",
    "acc = accuracy_score(valid_labels, preds)\n",
    "print(f\"Acc: {acc}\")\n",
    "print(f\"Skipped samples: {skipped_count}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Acc: 0.9380733944954128\n",
    "Skipped samples: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. 埋め込みに基づく感情分析\n",
    "\n",
    "\n",
    "事前学習済み言語モデルでテキストをベクトルで表現 (エンコード) し, そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "train_df = pd.read_table(\"../ch07/SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "dev_df = pd.read_table(\"../ch07/SST-2/dev.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['sentence']\n",
    "        label = int(self.data.iloc[idx]['label'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.encoder = GPT2Model.from_pretrained(\"gpt2-medium\")\n",
    "        hidden_size = self.encoder.config.n_embd\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = input_ids.shape[0]\n",
    "        selected_hidden_states = last_hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        logits = self.classifier(selected_hidden_states)\n",
    "        return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_preds.extend(preds.cpu().tolist())\n",
    "                val_true.extend(labels.cpu().tolist())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), \"best_gpt2_medium_sentiment_model.pt\")\n",
    "            print(f\"Model saved with accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_dataset = SSTDataset(train_df, tokenizer)\n",
    "    val_dataset = SSTDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SentimentClassifier()\n",
    "    model.to(device)\n",
    "\n",
    "    best_accuracy = train_model(model, train_loader, val_loader, device, epochs=3)\n",
    "    print(f\"Training completed with best validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Epoch 1/3 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2105/2105 [36:21<00:00,  1.04s/it]\n",
    "Epoch 1/3 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  3.01it/s]\n",
    "Epoch 1/3, Train Loss: 0.2461, Val Accuracy: 0.9300\n",
    "Model saved with accuracy: 0.9300\n",
    "Epoch 2/3 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2105/2105 [35:58<00:00,  1.03s/it]\n",
    "Epoch 2/3 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  3.03it/s]\n",
    "Epoch 2/3, Train Loss: 0.1375, Val Accuracy: 0.9358\n",
    "Model saved with accuracy: 0.9358\n",
    "Epoch 3/3 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2105/2105 [35:38<00:00,  1.02s/it]\n",
    "Epoch 3/3 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  3.03it/s]\n",
    "Epoch 3/3, Train Loss: 0.1015, Val Accuracy: 0.9369\n",
    "Model saved with accuracy: 0.9369\n",
    "Training completed with best validation accuracy: 0.9369\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ファインチューニング\n",
    "\n",
    "\n",
    "問題96のプロンプトに対して, 正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, set_seed\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    full_texts = [\n",
    "        \"Sentence: \" + sentence + \"\\nLabel: \" + str(label)\n",
    "        for sentence, label in zip(examples[\"sentence\"], examples[\"label\"])\n",
    "    ]\n",
    "    tokenized = tokenizer(full_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    labels = deepcopy(tokenized[\"input_ids\"])\n",
    "    \n",
    "    prompt_texts = [\n",
    "        \"Sentence: \" + sentence + \"\\nLabel:\" \n",
    "        for sentence in examples[\"sentence\"]\n",
    "    ]\n",
    "    prompt_tokenized = tokenizer(prompt_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    for i, prompt_ids in enumerate(prompt_tokenized[\"input_ids\"]):\n",
    "        prompt_len = len(prompt_ids)\n",
    "        labels[i][:prompt_len] = [-100] * prompt_len\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "def train_model(model, datasets, training_args, data_collator=None, output_dir=None):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"validation\"],\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def eval_model(model, datasets, max_new_tokens=2, max_length=None, num_return_sequences=1, skip_special_tokens=True):\n",
    "    input_texts = [f\"Sentence: {ex['sentence']}\\nLabel:\" for ex in datasets]\n",
    "    labels = [str(ex['label']) for ex in datasets]\n",
    "    preds = []\n",
    "    for input_text in tqdm(input_texts, desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        label_start = generated_text.find(\"Label:\") + len(\"Label:\")\n",
    "        pred = generated_text[label_start:].strip().split()[0]\n",
    "        preds.append(pred)\n",
    "        \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Acc: {acc}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../ch07/SST-2/train.tsv\", \"validation\": \"../ch07/SST-2/dev.tsv\"}, delimiter='\\t')\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_accumulation_steps=10\n",
    ")\n",
    "   \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "model_output_dir = \"./results/model\"\n",
    "    \n",
    "train_model(model, datasets=tokenized_datasets, training_args=training_args, data_collator=data_collator, output_dir=model_output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n",
    "   \n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../ch07/SST-2/train.tsv\", \"validation\": \"../ch07/SST-2/dev.tsv\"}, delimiter='\\t')\n",
    "eval_model(model, dataset[\"validation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Acc: 0.8635321100917431\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 選好チューニング\n",
    "\n",
    "\n",
    "問題96のプロンプトに対して, 正解の感情ラベルを含むテキストを望ましい応答, 間違った感情ラベルを含むテキストを望ましくない応答として, 事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ.選好チューニングのアルゴリズムとしては, 近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_prompt = tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized_chosen = tokenizer(examples[\"chosen\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized_rejected = tokenizer(examples[\"rejected\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"prompt_input_ids\": tokenized_prompt[\"input_ids\"],\n",
    "        \"chosen_input_ids\": tokenized_chosen[\"input_ids\"],\n",
    "        \"rejected_input_ids\": tokenized_rejected[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "def make_pref_dataset(examples):\n",
    "    records = {\"prompt\": [], \"chosen\": [], \"rejected\": []}\n",
    "    for sent, lbl in zip(examples[\"sentence\"], examples[\"label\"]):\n",
    "        prompt = f\"Sentence: {sent}\\nLabel:\"\n",
    "        chosen = f\"{prompt} {lbl}\"\n",
    "        wrong_lbl = 1 - lbl\n",
    "        rejected = f\"{prompt} {wrong_lbl}\"\n",
    "        records[\"prompt\"].append(prompt)\n",
    "        records[\"chosen\"].append(chosen)\n",
    "        records[\"rejected\"].append(rejected)\n",
    "    return records\n",
    "\n",
    "def train_model(model, train_dataset, eval_dataset, training_args, data_collator=None, compute_metrics=None, output_dir=None):\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def eval_model(model, datasets, max_new_tokens=2, max_length=None, num_return_sequences=1, skip_special_tokens=True):\n",
    "    input_texts = [f\"Sentence: {ex['sentence']}\\nLabel:\" for ex in datasets]\n",
    "    labels = [str(ex['label']) for ex in datasets]\n",
    "    preds = []\n",
    "    for input_text in tqdm(input_texts, desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        label_start = generated_text.find(\"Label:\") + len(\"Label:\")\n",
    "        pred = generated_text[label_start:].strip().split()[0]\n",
    "        preds.append(pred)\n",
    "        \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Acc: {acc}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "       \n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../ch07/SST-2/train.tsv\", \"validation\": \"../ch07/SST-2/dev.tsv\"}, delimiter='\\t')\n",
    "train_pref_dataset = dataset[\"train\"].map(make_pref_dataset, batched=True, remove_columns=dataset[\"train\"].column_names).map(tokenize_function, batched=True)\n",
    "eval_pref_dataset = dataset[\"validation\"].map(make_pref_dataset, batched=True, remove_columns=dataset[\"validation\"].column_names).map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500, \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    beta=0.05,\n",
    "    remove_unused_columns=True,\n",
    "    disable_tqdm=False,\n",
    "    eval_accumulation_steps=10\n",
    ")\n",
    "\n",
    "data_collator = DPODataCollatorWithPadding(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    label_pad_token_id=-100,\n",
    "    is_encoder_decoder=False\n",
    ")\n",
    "\n",
    "model_output_dir = \"./results/model_dop\"\n",
    "        \n",
    "train_model(model, train_dataset=train_pref_dataset, eval_dataset=eval_pref_dataset, training_args=training_args, data_collator=data_collator, output_dir=model_output_dir)\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n",
    "        \n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../ch07/SST-2/train.tsv\", \"validation\": \"../ch07/SST-2/dev.tsv\"}, delimiter='\\t')\n",
    "eval_model(model, dataset[\"validation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Acc: 0.9139908256880734\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
